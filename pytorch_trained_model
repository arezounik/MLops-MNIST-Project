import mlflow
import numpy as np
import copy
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torchvision import datasets, transforms
from torch.optim.lr_scheduler import StepLR

lr = 1e-3
gamma = 0.1


train_loader = torch.utils.data.DataLoader(
    datasets.MNIST('./data', train=True, download=True,
                    transform=transforms.Compose([transforms.ToTensor()])), batch_size=64, shuffle=False, num_workers=4)

test_loader = torch.utils.data.DataLoader(
    datasets.MNIST('./data', train=False,
                   transform=transforms.Compose([transforms.ToTensor()])), batch_size=24, shuffle=False, num_workers=4)

device = torch.device('cpu')


def salt_and_pepper(data, prob = 1):

    prob = 1

    newdata = torch.empty_like(data).copy_(data)

    temp = torch.rand(newdata.size(dim=0), 1, 14, 28).to(device)
    mask = temp > 0.5

    rand = torch.randn(newdata.size(dim=0), 1, 14, 28).to(device)
    rand[rand < prob/2] = 0
    rand[rand > 1 - prob/2] = 1

    newdata[mask] = rand[mask]

    return newdata


class Semantic_encoder1(nn.Module):
    def __init__(self):
        super(Semantic_encoder1, self).__init__()

        self.Semantic_encoder1 = nn.Sequential(
                              nn.Linear(392,256),
                              nn.ReLU(),
                              nn.Linear(256,128),
                              nn.ReLU(),
                              nn.Linear(128,81),
                              nn.Sigmoid()
                            )

    def forward(self, view1):

        view1 = salt_and_pepper(view1, prob=1).to(device)

        view1 = torch.reshape(view1,(-1,392))

        V1 = self.Semantic_encoder1(view1)


        return V1

    def Semantic_encoder1_finetune(self, requires_grad = False):

        for params in self.Semantic_encoder1.parameters():
            params.requires_grad =  requires_grad


# Semantic_encoder1 = Semantic_encoder1()

class Semantic_encoder2(nn.Module):
    def __init__(self):
        super(Semantic_encoder2, self).__init__()

        self.Semantic_encoder2 = nn.Sequential(
                              nn.Linear(392,256),
                              nn.ReLU(),
                              nn.Linear(256,128),
                              nn.ReLU(),

                              nn.Linear(128,81),
                              nn.Sigmoid()
                            )

    def forward(self, view2):

        view2 = salt_and_pepper(view2, prob=1).to(device)

        view2 = torch.reshape(view2,(-1,392))

        V2 = self.Semantic_encoder2(view2)

        return V2

    def Semantic_encoder2_finetune(self, requires_grad = False):

        for params in self.Semantic_encoder2.parameters():
            params.requires_grad =  requires_grad

# Semantic_encoder2 = Semantic_encoder2()

class Autoencoder1(nn.Module):
    def __init__(self):
        super(Autoencoder1, self).__init__()

        self.encoder1 = nn.Sequential(
                      nn.Conv2d(1,32, kernel_size=5),
                      nn.ReLU(),
                      nn.Conv2d(32,6,kernel_size=5),
                      nn.ReLU(),
                      )


        self.decoder1 = nn.Sequential(
                      nn.ConvTranspose2d(6,32,kernel_size=5),
                      nn.ReLU(),
                      nn.ConvTranspose2d(32,1,kernel_size=5),
                      )


    def forward(self, V1):

        V1 = torch.reshape(V1, (V1.size(dim=0), 1 , 9 , 9))

        encoded_V1 = self.encoder1(V1)

        snr = 10

        x = torch.square(encoded_V1)

        aver1 = torch.sum(x) / torch.prod(torch.tensor(x.shape))
        aver_noise1 = aver1 * (1 / 10 **(snr/10))

        noise1 = (torch.randn(size=encoded_V1.shape).to(device)) * torch.sqrt(aver_noise1)

        encoded_V1noisy = encoded_V1 + noise1

        decoded_V1 = self.decoder1(encoded_V1noisy)

        decoded_V1 = torch.reshape(decoded_V1,(-1,81))

        return encoded_V1, encoded_V1noisy, decoded_V1, aver_noise1, aver1, noise1

    def Autoencoder1_finetune(self, requires_grad = False):

        for params in self.encoder1.parameters():
            params.requires_grad =  requires_grad

        for params in self.decoder1.parameters():
            params.requires_grad =  requires_grad


# Autoencoder1 = Autoencoder1()

class Autoencoder2(nn.Module):
    def __init__(self):
        super(Autoencoder2, self).__init__()

        self.encoder2 = nn.Sequential(
                      nn.Conv2d(1,32, kernel_size=5),
                      nn.ReLU(),
                      nn.Conv2d(32,6,kernel_size=5),
                      nn.ReLU(),
                      )


        self.decoder2 = nn.Sequential(
                      nn.ConvTranspose2d(6,32,kernel_size=5),
                      nn.ReLU(),
                      nn.ConvTranspose2d(32,1,kernel_size=5),
                      )



    def forward(self, V2):

        V2 = torch.reshape(V2, (V2.size(dim=0), 1 , 9 , 9))

        encoded_V2 = self.encoder2(V2)

        snr = 10

        x = torch.square(encoded_V2)

        aver2 = torch.sum(x) / torch.prod(torch.tensor(x.shape)).to(device)
        aver_noise2 = aver2 * (1 / 10 **(snr/10))

        noise2 = (torch.randn(size=encoded_V2.shape).to(device)) * torch.sqrt(aver_noise2)

        encoded_V2noisy = encoded_V2 + noise2

        decoded_V2 = self.decoder2(encoded_V2noisy)

        decoded_V2 = torch.reshape(decoded_V2,(-1,81))

        return encoded_V2, encoded_V2noisy, decoded_V2, aver_noise2, aver2, noise2

    def Autoencoder2_finetune(self, requires_grad = False):

        for params in self.encoder2.parameters():
            params.requires_grad =  requires_grad

        for params in self.decoder2.parameters():
            params.requires_grad =  requires_grad



# Autoencoder2 = Autoencoder2()

class Cross_view_encoding(nn.Module):
    def __init__(self):
        super(Cross_view_encoding, self).__init__()
        self.InternalNetwork1_encoder = nn.Sequential(
                      nn.Conv2d(1,32, kernel_size=5),
                      nn.ReLU(),
                      nn.Conv2d(32,32, kernel_size=5),
                      nn.ReLU(),
                      )
        self.InternalNetwork2_encoder = nn.Sequential(
                      nn.Conv2d(1,32, kernel_size=5),
                      nn.ReLU(),
                      nn.Conv2d(32,32, kernel_size=5),
                      nn.ReLU(),
                      )


    def forward(self, decoded_V1, decoded_V2):

        decoded_V1 = torch.reshape(decoded_V1, (decoded_V1.size(dim=0), 1 , 9 , 9))
        decoded_V2 = torch.reshape(decoded_V2, (decoded_V2.size(dim=0), 1 , 9 , 9))

        z1 = self.InternalNetwork1_encoder(decoded_V1)
        sub_z2 = self.InternalNetwork2_encoder(decoded_V2)

        z2 = self.InternalNetwork1_encoder(decoded_V2)
        sub_z1 = self.InternalNetwork2_encoder(decoded_V1)

        z1 = torch.reshape(z1,(-1,32))
        sub_z2 = torch.reshape(sub_z2,(-1,32))

        z2 = torch.reshape(z2,(-1,32))
        sub_z1 = torch.reshape(sub_z1,(-1,32))

        z_cat = torch.cat((z1,sub_z2),dim=1)
        z_sub_cat = torch.cat((z2,sub_z1),dim=1)


        return z1, sub_z2, z2, sub_z1, z_cat, z_sub_cat

# Cross_view_encoding = Cross_view_encoding()

class InternalNetwork1(nn.Module):
    def __init__(self):
        super(InternalNetwork1, self).__init__()

        self.projection1 = nn.Sequential(
                              nn.Linear(32,12),
                              nn.BatchNorm1d(12),
                              nn.ReLU(),
                            )
        self.prediction1 = nn.Sequential(
                              nn.Linear(12,3),
                              nn.BatchNorm1d(3),
                              nn.ReLU(),
                            )


    def forward(self, z1, z2):

        project1 = self.projection1(z1)
        project2 = self.projection1(z2)

        q1 = self.prediction1(project1)
        q2 = self.prediction1(project2)


        return q1, q2

# InternalNetwork1 = InternalNetwork1()

class InternalNetwork2 (nn.Module):
    def __init__(self):
        super(InternalNetwork2, self).__init__()

        self.projection2 = nn.Sequential(
                              nn.Linear(32,3),
                              nn.BatchNorm1d(3),
                              nn.ReLU(),
                            )

    def forward(self, sub_z1, sub_z2):

        p2 = self.projection2(sub_z2)
        p1 = self.projection2(sub_z1)

        return p1, p2

    def InternalNetwork2finetune(self, requires_grad = False):

        for params in self.projection2.parameters():
            params.requires_grad =  requires_grad

# InternalNetwork2 = InternalNetwork2()



class Classifier(nn.Module):
    def __init__(self):
        super(Classifier, self).__init__()


        self.concat_decoder = nn.Sequential(
                        nn.Linear(64, 32),
                        nn.ReLU()
                        )
        self.classifier = nn.Sequential(
                        nn.Linear(32, 10),
                        nn.ReLU()
                        )

    def forward(self, z_cat):

      T = self.concat_decoder(z_cat)
      T_output = self.classifier(T)


      return  F.log_softmax(T_output, dim=1)

    def classopt(self, requires_grad = False):

        for params in self.concat_decoder.parameters():
            params.requires_grad =  requires_grad

        for params in self.classifier.parameters():
            params.requires_grad =  requires_grad

# Classifier = Classifier()


model1 = Semantic_encoder1().to(device)
model2 = Semantic_encoder2().to(device)
model3 = Autoencoder1().to(device)
model4 = Autoencoder2().to(device)
model5 = Cross_view_encoding().to(device)
model6 = InternalNetwork1().to(device)
model7 = InternalNetwork2().to(device)
model8 = Classifier().to(device)

model1.load_state_dict(torch.load('pretrained/end0.5semanticencoder1', map_location=torch.device('cpu')))
model2.load_state_dict(torch.load('pretrained/end0.5semanticencoder2', map_location=torch.device('cpu')))
model3.load_state_dict(torch.load('pretrained/end0.5autoencoder1', map_location=torch.device('cpu')))
model4.load_state_dict(torch.load('pretrained/end0.5autoencoder2', map_location=torch.device('cpu')))
model5.load_state_dict(torch.load('pretrained/end0.5crossviewencoding', map_location=torch.device('cpu')))
model6.load_state_dict(torch.load('pretrained/end0.5internalnetwork1', map_location=torch.device('cpu')))
model7.load_state_dict(torch.load('pretrained/end0.5internalnetwork2', map_location=torch.device('cpu')))
model8.load_state_dict(torch.load('pretrained/end0.5classifier', map_location=torch.device('cpu')))

#creating Fullsystemmodel

class FullSystemModel(nn.Module):
    def __init__(self):
        super(FullSystemModel, self).__init__()
        self.semantic_encoder1 = model1
        self.semantic_encoder2 = model2
        self.autoencoder1 = model3
        self.autoencoder2 = model4
        self.reciever_part1 = model5
        self.reciever_part2 = model6
        self.reciever_part3 = model7
        self.classifier = model8
        
    def forward(self, data):
        view1 = data[:,:,0:14,:]
        view2 = data[:,:,14:,:]
        
        V1 = self.semantic_encoder1(view1)
        V2 = self.semantic_encoder2(view2)
        encoded_V1, encoded_V1noisy, decoded_V1, aver_noise1, aver1, noise1 = self.autoencoder1(V1)
        encoded_V2, encoded_V2noisy, decoded_V2, aver_noise2, aver2, noise2 = self.autoencoder2(V2)
        z1, sub_z2, z2, sub_z1, z_cat, z_sub_cat = self.reciever_part1(decoded_V1, decoded_V2)
        q1, q2 = self.reciever_part2(z1, z2)
        p1, p2 = self.reciever_part3(sub_z1, sub_z2)
        Tout = self.classifier(z_cat) 
        
        return Tout 

Full_systemmodel = FullSystemModel()

#creating test function

def test(Full_systemmodel, test_loader):

    Full_systemmodel.eval()


    test_loss = 0

    correct = 0
    correct1 = 0
    correct2 = 0


    with torch.no_grad():
        for data, label in test_loader:
            data, label = data.to(device), label.to(device)

            Tout = Full_systemmodel(data)

            test_loss += (F.nll_loss(Tout, label,  reduction='sum').item())

            pred = Tout.argmax(dim=1, keepdim=True)
            correct += pred.eq(label.view_as(pred)).sum().item()


    test_loss /= len(test_loader.dataset)


    return test_loss, 100. * (correct) / len(test_loader.dataset)

#Let's test

sample_input_numpy = None

for data, label in test_loader:
      data, label = data.to(device), label.to(device)
      loss, acc = test(Full_systemmodel, test_loader)
      sample_input_numpy = data.to(device).numpy()

      print(f"Loss: {loss}",f", Accuracy: {acc}")
      break

if sample_input_numpy is None:
    raise ValueError("test_loader is empty or failed to iterate. Cannot create MLflow signature.")


#MLFlow
import mlflow.pyfunc
import numpy as np
import pandas as pd
from mlflow.tracking import MlflowClient
import os
import shutil
from typing import Union


##creating_wrapper

class PyTorchTensorWrapper(mlflow.pyfunc.PythonModel):
    
    def load_context(self, context):

        try:
            model_path = context.artifacts["pytorch_model_path"]
            self.pytorch_model = mlflow.pytorch.load_model(model_path)
            self.pytorch_model.eval()
            
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            self.pytorch_model.to(self.device)
            print(f"Model loaded successfully on device: {self.device}")
            
        except Exception as e:
            print(f"Error loading PyTorch model artifact: {e}")
            raise

    def predict(self, context, model_input):

        if isinstance(model_input, pd.DataFrame):
            input_array = model_input.values
        elif isinstance(model_input, np.ndarray):
            input_array = model_input
        else:
            raise TypeError("Input data must be a NumPy array or pandas DataFrame.")
            
        input_tensor = torch.from_numpy(input_array).float().to(self.device)
        
        if input_tensor.dim() == 3:
            input_tensor = input_tensor.unsqueeze(1)
        
        with torch.no_grad():
            output_logits = self.pytorch_model(input_tensor)
        
        return output_logits.cpu().numpy()

#deploy



NEW_TRACKING_URI = "file:./mlruns"

mlflow.set_tracking_uri(NEW_TRACKING_URI)
print(f"✅ MLflow Tracking URI set to: {NEW_TRACKING_URI}")

DUMMY_METRICS = {"Accuracy": acc, "Loss": loss}

PYTORCH_FULL_MODEL_PATH = "full_system_model"
PYFUNC_MODEL_NAME = "full_system_pyfunc" 

mlflow.set_experiment("MNIST_Classification")


models_to_log = {
    "Semantic_encoder1": model1,
    "Semantic_encoder2": model2,
    "Autoencoder1": model3,
    "Autoencoder2": model4,
    "Cross_view_encoding": model5,
    "Internal_network1": model6,
    "Internal_network2": model7,
    "Classifier": model8
}

with mlflow.start_run(run_name="MNIST_Classification") as run:
    run_id = run.info.run_id
    
    mlflow.log_metrics(DUMMY_METRICS)
    print("✅ Metrics logged successfully within the new Run.")

    for name, component in models_to_log.items():
        mlflow.pytorch.log_model(
            pytorch_model=component,
            name=f"{name}" 
        )
    print("✅ 8 components logged successfully.")

    mlflow.pytorch.log_model(
        pytorch_model=Full_systemmodel,
        name=PYTORCH_FULL_MODEL_PATH, 
    )
    print("✅ Full PyTorch Model saved as artifact for Pyfunc.")
    
    mlflow.pyfunc.log_model(
        python_model=PyTorchTensorWrapper(), 
        name=PYFUNC_MODEL_NAME, 
        artifacts={
            "pytorch_model_path": f"runs:/{run_id}/{PYTORCH_FULL_MODEL_PATH}" 
        },
        input_example=sample_input_numpy, 
        registered_model_name="FINAL_DEPLOYABLE_MODEL" 
    )
    print("✅ Pyfunc Wrapper logged with correct signature and dependency.")
    
    mlflow.log_artifact(local_path="Readme")
    print("✅ Readme logged.")
    
    try:
        client = MlflowClient()
        client.transition_model_version_stage(
            name="FINAL_DEPLOYABLE_MODEL",
            version=1, 
            stage="Staging"
        )
        print("✅ Model transitioned to 'Staging' in Model Registry.")
    except Exception as e:
        print(f"Warning: Could not transition model stage (maybe not using remote registry): {e}")

    print(f"\n✨ Log Finished! Model 'FINAL_DEPLOYABLE_MODEL' registered.")


